{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE524Ir38H2e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB as GB\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import classification_report as CR\n",
        "from sklearn.metrics import confusion_matrix as CF\n",
        "from sklearn.metrics import f1_score\n",
        "from math import log\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3E_USCB_UZV"
      },
      "source": [
        "Load data and split it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edYwVVUQ-Ejh",
        "outputId": "ccef6af6-56b5-4053-9e8d-91276629e9a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000, 784) (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "MNIIST=mnist.load_data()\n",
        "#x,y = MNIIST[\"data\"], MNIIST[\"target\"]  ?????\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.2)\n",
        "print(x_train.shape,x_test.shape)\n",
        "re_x_train=x_train.reshape(60000,784)\n",
        "re_x_test=x_test.reshape(10000,784)\n",
        "print(re_x_train.shape,re_x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfmU-Z07Bjfo"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "x = mnist.data\n",
        "y = mnist.target\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(x, y, test_size = 0.2)\n",
        "print(x_train.shape,x_test.shape)\n",
        "Xtrain = X_train.reshape(60000,784)\n",
        "Xtest = X_test.reshape(10000,784)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm0fWPqM_aGU"
      },
      "source": [
        "Naive Bayes classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhhR8_aR_TJ-"
      },
      "outputs": [],
      "source": [
        "def caclculate_mean(x,y):\n",
        "    #(rows,cols)=x.shape\n",
        "    mean = np.zeros((len(np.unique(y)),  x.shape[1]))\n",
        "    for i,j in enumerate(np.unique(y)):\n",
        "         #row=axis(0)\n",
        "         mean[i]=(x[y==j]).mean(axis=0)\n",
        "    return mean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def caclculate_variance(x,y):\n",
        "    #(rows,cols)=x.shape\n",
        "    Variance = np.zeros((len(np.unique(y)),  x.shape[1]))\n",
        "    for i,j in enumerate(np.unique(y)):\n",
        "         #row=(axis=0)\n",
        "         Variance[i]=(x[y==j]).var(axis=0)\n",
        "    return Variance\n",
        "#caclculate_variance(X_train,Y_train)"
      ],
      "metadata": {
        "id": "rsKrg0rDRMPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caclculate_Probability_of_class(x,y):\n",
        "    #(rows,cols)=x.shape\n",
        "    Probability_of_class=np.zeros((len(np.unique(y))))\n",
        "    for i,j in enumerate(np.unique(y)):\n",
        "         #row=(axis=0)\n",
        "         Probability_of_class[i]= ((x[y==j]).shape[0])/x.shape[0]\n",
        "    return Probability_of_class\n"
      ],
      "metadata": {
        "id": "ak91YIUBR_Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_Liklihood(mean,Variance,x):\n",
        "     Numerator = np.exp(-((x-mean)**2) / (2*Variance))\n",
        "     Denominator = np.sqrt(2 *np.pi*Variance)\n",
        "     liklihood=Numerator/Denominator\n",
        "     return liklihood"
      ],
      "metadata": {
        "id": "vQ8naiqEOH7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, mean, Variance, Probability_of_class, classes):\n",
        "    posteriors = []\n",
        "    for x in X:\n",
        "        likelihood = [calculate_Liklihood(mean[i], Variance[i], x) for i in range(len(classes))]\n",
        "        log_likelihood = [np.sum(np.nan_to_num(np.log(z))) for z in likelihood]\n",
        "        log_Probability_of_class = [np.log(Probability_of_class[i]) for i in range(len(classes))]\n",
        "        posteriors_i = [log_likelihood[i] + log_Probability_of_class[i] for i in range(len(classes))]\n",
        "        posteriors.append(posteriors_i)\n",
        "    return [classes[np.argmax(posterior_i)] for posterior_i in posteriors]"
      ],
      "metadata": {
        "id": "fwkT6d9cXktS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n",
        "mean=caclculate_mean(x_train, y_train)\n",
        "Variance=caclculate_variance(x_train, y_train)\n",
        "Probability_of_class=caclculate_Probability_of_class(x_train, y_train)\n",
        "classes = np.unique(y_train)\n",
        "predictions = predict(x_test, mean, Variance, Probability_of_class, classes)\n",
        "print(\"classification_report\",CR(y_test, predictions))\n",
        "print(\"f1_score\",f1_score(y_test, predictions, average=None))\n",
        "print(\"confusion_matrix \",CF(y_test, predictions))\n",
        "accuracy = (predictions == y_test).mean()\n",
        "print(\"Accuracyof the model:\", (accuracy*100),\"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IzWF1gmYTbK",
        "outputId": "bac4e704-cc36-4d1a-8874-91062e758140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-1e1fa44a705b>:2: RuntimeWarning: divide by zero encountered in true_divide\n",
            "  Numerator = np.exp(-((x-mean)**2) / (2*Variance))\n",
            "<ipython-input-5-1e1fa44a705b>:2: RuntimeWarning: invalid value encountered in true_divide\n",
            "  Numerator = np.exp(-((x-mean)**2) / (2*Variance))\n",
            "<ipython-input-5-1e1fa44a705b>:4: RuntimeWarning: invalid value encountered in true_divide\n",
            "  liklihood=Numerator/Denominator\n",
            "<ipython-input-26-bc02b32ead22>:5: RuntimeWarning: divide by zero encountered in log\n",
            "  log_likelihood = [np.sum(np.nan_to_num(np.log(z))) for z in likelihood]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification_report               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.87       980\n",
            "           1       0.81      0.96      0.88      1135\n",
            "           2       0.90      0.45      0.60      1032\n",
            "           3       0.79      0.64      0.70      1010\n",
            "           4       0.87      0.35      0.50       982\n",
            "           5       0.69      0.15      0.24       892\n",
            "           6       0.72      0.92      0.81       958\n",
            "           7       0.88      0.42      0.57      1028\n",
            "           8       0.38      0.68      0.49       974\n",
            "           9       0.42      0.92      0.58      1009\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.73      0.64      0.62     10000\n",
            "weighted avg       0.73      0.65      0.63     10000\n",
            "\n",
            "f1_score [0.86650367 0.87710843 0.60116354 0.70394737 0.50361795 0.24236818\n",
            " 0.80730594 0.57124097 0.49260355 0.57809583]\n",
            "confusion_matrix  [[ 886    1    3    4    3    8   29    2   30   14]\n",
            " [   0 1092    2    3    0    1    7    0   28    2]\n",
            " [  44   33  465   81    7    3  185    7  192   15]\n",
            " [  20   45   13  642    3    6   30   11  162   78]\n",
            " [  12    8    9    2  348   10   48    9  101  435]\n",
            " [  70   35    5   47   12  131   33    6  474   79]\n",
            " [  13   17    7    0    1   10  884    0   25    1]\n",
            " [   0   18    4   12    9    3    4  435   31  512]\n",
            " [  14   93    4   15    8   16   11    7  666  140]\n",
            " [   6   13    3    8    9    1    1   18   21  929]]\n",
            "Accuracyof the model: 64.78 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}